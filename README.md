# ğŸ“š Automated Book Workflow

An AI-powered pipeline to **scrape, rewrite, review, and publish book chapters** automatically. This project demonstrates a modern, agent-based content automation system using Python, Playwright, and ChromaDB.

---

## âœ¨ Features

- ğŸ¤– **Agent-Based Architecture**  
  Modular agents handle each part of the workflow â€” scraping, rewriting, reviewing, and versioning.

- ğŸŒ **Web Content Scraping**  
  Uses [Playwright](https://playwright.dev/python/) to scrape chapters and capture full-page screenshots.

- âœï¸ **AI-Powered Content Generation**  
  Simulates AI agents for rewriting and refining content using LLMs.

- ğŸ§‘â€ğŸ’» **Human-in-the-Loop (HITL)**  
  Reviewer can approve or edit AI-generated content manually via CLI.

- ğŸ’¾ **Vector-Based Versioning**  
  Final versions are stored in [ChromaDB](https://docs.trychroma.com/), enabling semantic search.

- ğŸ§  **Reinforcement Learning Concept**  
  Assigns reward scores to AI outputs based on how little the human had to edit.

- ğŸš€ **API-Ready**  
  A sample [FastAPI](https://fastapi.tiangolo.com/) server exposes the workflow as an HTTP API.

---

## âš™ï¸ How It Works

The workflow is managed by a central orchestrator script (`main.py`) that runs each agent in sequence:

1. **Scraping Agent**  
   â†’ Takes a URL, scrapes the chapter's text, saves a screenshot.

2. **AI Writer Agent**  
   â†’ Rewrites the raw text using creative prompts.

3. **AI Reviewer Agent**  
   â†’ Refines grammar, clarity, and style.

4. **HITL Agent (Human-in-the-Loop)**  
   â†’ CLI prompt allows human to approve, edit, or reject.

5. **Versioning Agent**  
   â†’ Calculates an RL-style reward score  
   â†’ Saves final text + metadata to ChromaDB

6. **Search**  
   â†’ Search by semantic similarity or highest reward score.

---

## ğŸ› ï¸ Tech Stack

| Category           | Tool                         |
|--------------------|------------------------------|
| Backend            | Python                       |
| Web Scraping       | Playwright                   |
| AI Text Handling   | OpenAI / LLMs (simulated)    |
| Database           | ChromaDB (Vector Store)      |
| API Layer          | FastAPI, Uvicorn             |
| Text Comparison    | python-Levenshtein           |
| HTML Parsing       | BeautifulSoup4               |

---

## ğŸš€ Setup & Usage

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git
cd YOUR_REPOSITORY_NAME
```

### 2. Create a Virtual Environment

**On Windows:**
```powershell
python -m venv venv
venv\Scripts\activate
```

**On macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install Playwright Browsers

```bash
playwright install
```

### 5. Run the Workflow

```bash
python main.py
```

The workflow will:
- Scrape a chapter from a URL
- Rewrite and refine it using AI agents
- Let you review it in the CLI
- Save outputs (text, screenshots, vector data) in the `outputs/` directory

---

## ğŸ“ Project Structure

```
/automated-book-workflow
â”‚
â”œâ”€â”€ main.py                # The main orchestrator script
â”œâ”€â”€ agents/                # Specialized agents
â”‚   â”œâ”€â”€ scraping_agent.py
â”‚   â”œâ”€â”€ ai_agents.py
â”‚   â”œâ”€â”€ hitl_agent.py
â”‚   â”œâ”€â”€ versioning_agent.py
â”‚
â”œâ”€â”€ api/                   # Optional FastAPI service
â”‚   â””â”€â”€ server.py
â”‚
â”œâ”€â”€ outputs/               # Generated content
â”‚   â”œâ”€â”€ /screenshots
â”‚   â””â”€â”€ /chapters
â”‚
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .gitignore             # Git ignore rules
â””â”€â”€ README.md              # You are here!
```

---
